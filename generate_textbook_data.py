#!/usr/bin/env python3
"""
Dynamic Chinese Textbook Data Generator with Location-Aware URL Testing
Fetches textbook data directly from GitHub repository: https://github.com/TapXWorld/ChinaTextbook
Uses GitHub Trees API for efficient single-call repository structure retrieval.
Automatically discovers subjects, publishers, grades, and PDF files including split files.
Tests jsDelivr CDN compatibility for each PDF and generates location-aware URLs.
Supports flexible configuration via textbook-config.json file.
"""

import json
import os
import re
import requests
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import urllib.parse
import fnmatch
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

class URLTester:
    """URL testing class for jsDelivr CDN compatibility and fallback generation"""
    
    def __init__(self, max_workers: int = 10):
        """Initialize URL tester
        
        Args:
            max_workers (int): Maximum number of concurrent threads for testing
        """
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
        # URL templates
        self.jsdelivr_template = "https://cdn.jsdelivr.net/gh/TapXWorld/ChinaTextbook@master/{}"
        self.fallback_template = "https://ghfast.top/https://raw.githubusercontent.com/TapXWorld/ChinaTextbook/master/{}"
    
    def test_jsdelivr_link(self, file_path: str) -> Tuple[str, bool, int, str]:
        """Test a single jsDelivr link
        
        Args:
            file_path (str): The file path in the repository
            
        Returns:
            Tuple[str, bool, int, str]: (file_path, success, status_code, error_message)
        """
        encoded_path = urllib.parse.quote(file_path, safe='/')
        jsdelivr_url = self.jsdelivr_template.format(encoded_path)
        
        try:
            # Use HEAD request to avoid downloading the entire file
            response = self.session.head(jsdelivr_url, timeout=10)
            success = response.status_code == 200
            return file_path, success, response.status_code, ""
        except requests.exceptions.RequestException as e:
            return file_path, False, 0, str(e)
    
    def get_optimal_urls(self, github_url: str, file_path: str) -> Dict[str, str]:
        """Get optimal URLs for international and China users with real-time testing
        
        Args:
            github_url (str): Original GitHub raw URL
            file_path (str): File path in repository for testing
            
        Returns:
            Dict[str, str]: Dictionary with international_url and china_url
        """
        result = {
            'international_url': github_url,  # International users always use GitHub direct
            'china_url': github_url  # Default fallback
        }
        
        # Test jsDelivr for China users
        _, jsdelivr_works, status_code, error_msg = self.test_jsdelivr_link(file_path)
        
        if jsdelivr_works:
            # jsDelivr works, use it for China users
            encoded_path = urllib.parse.quote(file_path, safe='/')
            result['china_url'] = self.jsdelivr_template.format(encoded_path)
        else:
            # jsDelivr failed, use fallback proxy for China users
            encoded_path = urllib.parse.quote(file_path, safe='/')
            result['china_url'] = self.fallback_template.format(encoded_path)
        
        return result, jsdelivr_works, status_code
    
    def test_multiple_urls(self, url_data: List[Tuple[str, str]]) -> Dict[str, Dict]:
        """Test multiple URLs concurrently
        
        Args:
            url_data: List of (github_url, file_path) tuples
            
        Returns:
            Dict mapping github_url to url results and test info
        """
        results = {}
        total_files = len(url_data)
        completed = 0
        
        print(f"ğŸš€ Testing {total_files} jsDelivr links with {self.max_workers} threads...")
        print("â³ This may take a few minutes...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_data = {
                executor.submit(self.get_optimal_urls, github_url, file_path): (github_url, file_path)
                for github_url, file_path in url_data
            }
            
            # Process completed tasks
            for future in as_completed(future_to_data):
                completed += 1
                github_url, file_path = future_to_data[future]
                
                try:
                    url_result, jsdelivr_works, status_code = future.result()
                    results[github_url] = {
                        'urls': url_result,
                        'jsdelivr_works': jsdelivr_works,
                        'status_code': status_code,
                        'file_path': file_path
                    }
                    
                    status_icon = "âœ…" if jsdelivr_works else "âŒ"
                    print(f"{status_icon} [{completed}/{total_files}] {file_path}")
                    
                except Exception as e:
                    # Fallback in case of error
                    encoded_path = urllib.parse.quote(file_path, safe='/')
                    fallback_url = self.fallback_template.format(encoded_path)
                    results[github_url] = {
                        'urls': {
                            'international_url': github_url,
                            'china_url': fallback_url
                        },
                        'jsdelivr_works': False,
                        'status_code': 0,
                        'file_path': file_path,
                        'error': str(e)
                    }
                    print(f"âŒ [{completed}/{total_files}] {file_path} (Error: {e})")
                
                # Progress update and rate limiting
                if completed % 50 == 0:
                    print(f"ğŸ“Š Progress: {completed}/{total_files} ({completed/total_files*100:.1f}%)")
                    time.sleep(1)
        
        # Print summary
        working_count = sum(1 for r in results.values() if r['jsdelivr_works'])
        failed_count = total_files - working_count
        success_rate = (working_count / total_files * 100) if total_files > 0 else 0
        
        print(f"\nğŸ“Š URL Testing Summary:")
        print(f"âœ… jsDelivr working: {working_count}")
        print(f"âŒ jsDelivr failed: {failed_count}")
        print(f"ğŸ“ˆ Success rate: {success_rate:.1f}%")
        
        return results

class GitHubTextbookScanner:
    """GitHub Textbook Scanner class for fetching and parsing Chinese textbook data.
    
    This class is specifically designed for the TapXWorld/ChinaTextbook repository:
    https://github.com/TapXWorld/ChinaTextbook
    
    The parsing logic is hardcoded for Chinese educational materials and will NOT work
    with other repositories due to:
    - Chinese filename parsing patterns
    - Chinese education level structure (å°å­¦/åˆä¸­/é«˜ä¸­/å¤§å­¦)
    - Chinese subject recognition (è¯­æ–‡/æ•°å­¦/è‹±è¯­ etc.)
    - Chinese publisher naming conventions
    
    Attributes:
        owner (str): Fixed as "TapXWorld"
        repo (str): Fixed as "ChinaTextbook"
        token (Optional[str]): GitHub API token for authentication
        trees_api_url (str): GitHub Trees API endpoint for ChinaTextbook repo
        raw_base_url (str): Base URL for accessing raw files from ChinaTextbook repo
        session (requests.Session): Requests session for API calls
        cache_file (str): Local cache file path for repository tree data
        config (Dict): Configuration settings loaded from textbook-config.json
        url_tester (URLTester): URL testing instance for jsDelivr compatibility
    """

    def __init__(self, token: Optional[str] = None, test_urls: bool = True):
        """Initialize GitHubTextbookScanner for TapXWorld/ChinaTextbook repository.

        Args:
            token (Optional[str], optional): GitHub API token for higher rate limits. 
                                           Defaults to None.
            test_urls (bool, optional): Whether to test URLs for jsDelivr compatibility. Defaults to True.
        """
        # Hardcoded for TapXWorld/ChinaTextbook repository only
        self.owner = "TapXWorld"
        self.repo = "ChinaTextbook"
        self.token = token
        self.test_urls = test_urls
        
        # Use default configuration (no external config file needed)
        self.config = self.get_default_config()
        
        # Fixed API URLs for the specific repository
        self.trees_api_url = f"https://api.github.com/repos/{self.owner}/{self.repo}/git/trees/master"
        self.raw_base_url = f"https://raw.githubusercontent.com/{self.owner}/{self.repo}/master"
        self.session = requests.Session()
        
        # Initialize URL tester if testing is enabled
        # url tester is to test jsDelivr compatibility for each PDF
        if self.test_urls:
            self.url_tester = URLTester(max_workers=15)  # Increased workers for faster testing
            print("ğŸ”— URL testing enabled - will test jsDelivr compatibility for each PDF")
        else:
            self.url_tester = None
            print("ğŸ”— URL testing disabled - using GitHub URLs only")
        
        # Set up authentication if token provided
        if token:
            self.session.headers.update({'Authorization': f'token {token}'})
            print("ğŸ”‘ Using GitHub token for higher rate limits")
        
        # Fixed cache file for the specific repository
        self.cache_file = "textbook-tree-cache.json"
                
    def get_default_config(self) -> Dict:
        """Get default configuration (no external config file needed)"""
        config = {
            "levels": {
                "xiaoxue": {"enabled": True, "ignorePatterns": [], "showGrades": True, "requireGrades": False},
                "chuzhong": {"enabled": True, "ignorePatterns": [], "showGrades": True, "requireGrades": False},
                "gaozhong": {"enabled": True, "ignorePatterns": [], "showGrades": True, "requireGrades": False},
                "daxue": {"enabled": True, "ignorePatterns": [], "showGrades": False, "requireGrades": False},
                "xiaoxue45": {"enabled": True, "ignorePatterns": [], "showGrades": True, "requireGrades": False},
                "chuzhong45": {"enabled": True, "ignorePatterns": [], "showGrades": True, "requireGrades": False}
            }
        }
        return config

    def should_ignore_path(self, path: str, level_key: str) -> bool:
        """Check if a path should be ignored based on configuration"""
        if level_key not in self.config.get("levels", {}):
            return False
            
        level_config = self.config["levels"][level_key]
        if not level_config.get("enabled", True):
            return True
            
        ignore_patterns = level_config.get("ignorePatterns", [])
        for pattern in ignore_patterns:
            if fnmatch.fnmatch(path, pattern):
                print(f"ğŸš« Ignoring path: {path} (matches pattern: {pattern})")
                return True
        return False

    def get_level_key_from_path(self, path: str) -> Optional[str]:
        """Map path to configuration level key"""
        path_parts = path.split('/')
        if not path_parts:
            return None
            
        level_name = path_parts[0]
        level_mapping = {
            "å°å­¦": "xiaoxue",
            "åˆä¸­": "chuzhong", 
            "é«˜ä¸­": "gaozhong",
            "å¤§å­¦": "daxue",
            "å°å­¦45å­¦åˆ¶": "xiaoxue45",
            "åˆä¸­45å­¦åˆ¶": "chuzhong45"
        }
        
        return level_mapping.get(level_name)

    def should_show_grades_for_level(self, level_key: str) -> bool:
        """Check if grades should be shown for a specific level"""
        if level_key not in self.config.get("levels", {}):
            return True
            
        level_config = self.config["levels"][level_key]
        return level_config.get("showGrades", True)


        
    def get_repository_tree(self) -> Optional[List[Dict]]:
        """Get entire repository tree structure in one API call"""
        
        # Try to load from cache first
        tree_data = self.load_tree_cache()
        if tree_data:
            print(f"ğŸ“‚ Loaded repository tree from cache textbook-tree-cache.json - ({len(tree_data)} entries)")
            return tree_data
        
        print("ğŸŒ³ Fetching repository tree structure from GitHub API...")
        
        try:
            # Make API call with recursive=1 to get all files
            response = self.session.get(f"{self.trees_api_url}?recursive=1")
            
            if response.status_code == 200:
                data = response.json()
                tree_data = data.get('tree', [])
                print(f"âœ… Successfully fetched {len(tree_data)} tree entries from GitHub API")
                
                # Save to cache
                self.save_tree_cache(tree_data)
                return tree_data
                
            elif response.status_code == 403:
                print("âŒ Rate limited or access denied")
                return None
            else:
                print(f"âŒ Error fetching tree: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ Exception fetching tree: {e}")
            return None
    
    def save_tree_cache(self, tree_data: List[Dict]):
        """Save tree data to cache file"""
        try:
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'tree': tree_data
            }
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ Saved tree cache to {self.cache_file}")
        except Exception as e:
            print(f"âš ï¸ Could not save cache: {e}")
    
    def load_tree_cache(self) -> Optional[List[Dict]]:
        """Load tree data from cache file"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                return cache_data.get('tree', [])
        except Exception as e:
            print(f"âš ï¸ Could not load cache: {e}")
        return None
    
    def _parse_textbook_metadata(self, filename: str, path: str = None) -> Dict[str, str]:
        """Parse textbook path and filename using systematic pattern recognition
        
        Handles all directory structure patterns in TapXWorld/ChinaTextbook repository:
        - Pattern A: {level}/{subject}/{publisher}/{direct_pdf_file}
        - Pattern B: {level}/{subject}/{publisher}/{grade}/{pdf_file_or_merge_folder}
        - Pattern C: Split files in _merge_folder directories
        - Pattern D: Math practice materials in special directory
        - Pattern E: {level}/{subject}/{direct_pdf_file} (no publisher, common in university level)
        - Special: Five-four education systems
        - Enhanced: Complex filenames with special handling
        
        Args:
            filename (str): The PDF filename to parse
            path (str, optional): Full file path for structured parsing
            
        Returns:
            Dict[str, str]: Complete metadata including all textbook information
        """
        # Initialize default values
        result = {
            'original_name': filename,
            'parsed_name': filename.replace('.pdf', '').replace('.PDF', ''),
            'subject': 'unknown',
            'grade': 'unknown', 
            'semester': 'unknown',
            'level': 'unknown',
            'publisher': 'æœªçŸ¥å‡ºç‰ˆç¤¾',
            'part_number': None,
            'is_split': False
        }
        
        # Handle split files first (applies to all patterns)
        name = result['parsed_name']
        split_match = re.search(r'\.(\d+)$', name)
        if split_match:
            result['part_number'] = int(split_match.group(1))
            result['parsed_name'] = name[:split_match.start()]
            result['is_split'] = True
        
        # If no path provided, fall back to filename-only parsing
        if not path:
            return self._parse_from_filename_only(result)
        
        # Split path and determine pattern
        path_parts = path.split('/')
        
        # Pattern D: Math practice materials
        if path.startswith('å­¦æ•°å­¦æœ€é‡è¦çš„åˆ·ä¹ é¢˜åœ¨è¿™é‡Œ/'):
            print(f"ğŸ§® Processing math practice file: {path}")
            return self._parse_math_practice_pattern(path_parts, result)
        
        # Patterns A, B, C: Regular textbooks
        elif len(path_parts) >= 3:
            parsed_result = self._parse_regular_textbook_pattern(path_parts, result)
            
            # If regular parsing didn't extract enough information, try special handling
            if (parsed_result['subject'] == 'unknown' or 
                parsed_result['grade'] == 'unknown' or 
                parsed_result['grade'] == result['parsed_name']):  # Grade equals filename indicates parsing failure
                
                special_result = self._parse_special_complex_filename(result['parsed_name'], path)
                if special_result['subject'] != 'unknown':
                    # Merge special parsing results, keeping path-based level if available
                    for key, value in special_result.items():
                        if value != 'unknown' and value != 'æœªçŸ¥å‡ºç‰ˆç¤¾':
                            if key == 'level' and parsed_result['level'] != 'unknown':
                                continue  # Keep path-based level
                            parsed_result[key] = value
            
            return parsed_result
        
        # Invalid/unknown pattern - fall back to filename parsing
        else:
            return self._parse_from_filename_only(result)
    
    def _parse_regular_textbook_pattern(self, path_parts: List[str], result: Dict[str, str]) -> Dict[str, str]:
        """Parse regular textbook patterns A, B, and C"""
        
        # Extract level (first component)
        result['level'] = path_parts[0] if path_parts[0] else 'unknown'
        
        # Check configuration for grade handling for this level
        level_key = self.get_level_key_from_path('/'.join(path_parts))
        show_grades = self.should_show_grades_for_level(level_key) if level_key else True
        
        # Extract subject (second component)  
        result['subject'] = path_parts[1] if len(path_parts) > 1 and path_parts[1] else 'unknown'
        
        # Determine if third component is publisher or direct file
        if len(path_parts) >= 3:
            third_component = path_parts[2]
            
            # Check if third component is a PDF file or merge folder (no publisher case)
            if (third_component.endswith('.pdf') or 
                re.search(r'\.pdf\.\d+$', third_component) or  # Split PDF files
                '.pdf_merge_folder' in third_component or
                third_component.startswith('ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦')):
                # Pattern: {level}/{subject}/{direct_pdf_file} - no publisher
                result['publisher'] = 'æœªçŸ¥å‡ºç‰ˆç¤¾'
                if show_grades:
                    # Try special complex filename parsing first
                    special_result = self._parse_special_complex_filename(result['parsed_name'])
                    if special_result['grade'] != 'unknown':
                        result['grade'] = special_result['grade']
                    else:
                        result['grade'] = self._extract_grade_from_name(result['parsed_name']) or 'unknown'
                else:
                    result['grade'] = 'course'  # For university level, use 'course' instead of grade
            else:
                # Third component is publisher
                result['publisher'] = self._clean_publisher_name(third_component)
                
                # Determine if we have Pattern A (direct files) or Pattern B (grade directories)
                if len(path_parts) >= 4:
                    fourth_component = path_parts[3]
                    
                    # Check if fourth component is a file or merge folder (Pattern A)
                    if (fourth_component.endswith('.pdf') or 
                        re.search(r'\.pdf\.\d+$', fourth_component) or  # Split PDF files
                        '.pdf_merge_folder' in fourth_component or
                        fourth_component.startswith('ä¹‰åŠ¡æ•™è‚²æ•™ç§‘ä¹¦')):
                        # Pattern A: Direct PDF file, extract grade from filename
                        if show_grades:
                            # Try special complex filename parsing first
                            special_result = self._parse_special_complex_filename(result['parsed_name'])
                            if special_result['grade'] != 'unknown':
                                result['grade'] = special_result['grade']
                            else:
                                result['grade'] = self._extract_grade_from_name(result['parsed_name']) or 'unknown'
                        else:
                            result['grade'] = 'course'
                    else:
                        # Pattern B: Grade directory exists
                        if show_grades:
                            result['grade'] = fourth_component
                        else:
                            result['grade'] = 'course'
                else:
                    # Only three components: {level}/{subject}/{publisher} - unusual case
                    result['grade'] = 'course' if not show_grades else 'unknown'
        
        # Extract semester from filename
        result['semester'] = self._extract_semester_from_name(result['parsed_name']) or 'unknown'
        
        return result
    
    def _parse_math_practice_pattern(self, path_parts: List[str], result: Dict[str, str]) -> Dict[str, str]:
        """Parse math practice materials (Pattern D) - assign to appropriate education levels"""
        
        # Default practice values
        result['subject'] = 'æ•°å­¦ç»ƒä¹ '
        result['grade'] = 'ç»ƒä¹ é¢˜'
        result['publisher'] = 'ç»ƒä¹ é¢˜é›†'
        result['semester'] = 'practice'
        
        # Assign to appropriate education level based on path
        if len(path_parts) >= 2:
            second_part = path_parts[1]  # e.g., "åˆä¸­ç»ƒä¹ é¢˜_å¸¦ç­”æ¡ˆ"
            
            if 'åˆä¸­ç»ƒä¹ é¢˜' in second_part:
                result['level'] = 'åˆä¸­'
                # Determine if it's ä¸­è€ƒ (middle school entrance exam)
                full_path = '/'.join(path_parts)
                if 'ä¸­è€ƒ' in result['original_name'] or 'ä¸­è€ƒ' in full_path:
                    result['subject'] = 'ä¸­è€ƒæ•°å­¦'
                    result['grade'] = 'ä¸­è€ƒç»ƒä¹ '
                else:
                    result['subject'] = 'æ•°å­¦ç»ƒä¹ '
                    result['grade'] = self._extract_grade_from_name(result['parsed_name']) or 'æ•°å­¦ç»ƒä¹ '
                    
            elif 'é«˜ä¸­ç»ƒä¹ é¢˜' in second_part:
                result['level'] = 'é«˜ä¸­'
                # Determine if it's é«˜è€ƒ (college entrance exam)
                full_path = '/'.join(path_parts)
                if 'é«˜è€ƒ' in result['original_name'] or 'é«˜è€ƒ' in full_path:
                    result['subject'] = 'é«˜è€ƒæ•°å­¦'
                    result['grade'] = 'é«˜è€ƒç»ƒä¹ '
                else:
                    result['subject'] = 'æ•°å­¦ç»ƒä¹ '
                    result['grade'] = self._extract_grade_from_name(result['parsed_name']) or 'æ•°å­¦ç»ƒä¹ '
                    
            elif 'å°å­¦ç»ƒä¹ é¢˜' in second_part:
                result['level'] = 'å°å­¦'
                result['subject'] = 'æ•°å­¦ç»ƒä¹ '
                result['grade'] = self._extract_grade_from_name(result['parsed_name']) or 'æ•°å­¦ç»ƒä¹ '
            else:
                # Fallback - if we can't determine level, default to åˆä¸­
                result['level'] = 'åˆä¸­'
        else:
            # Fallback for incomplete paths
            result['level'] = 'åˆä¸­'
        
        # Try to extract more specific publisher from test name (third component)
        if len(path_parts) >= 3:
            test_name = path_parts[2]
            # If test name contains publisher info, use it
            if any(indicator in test_name for indicator in ['ç‰ˆ', 'å‡ºç‰ˆç¤¾']):
                result['publisher'] = self._clean_publisher_name(test_name)
            else:
                # Use test name as a more specific identifier
                result['publisher'] = test_name if test_name else 'ç»ƒä¹ é¢˜é›†'
        
        print(f"ğŸ§® Math practice parsed: level={result['level']}, subject={result['subject']}, grade={result['grade']}")
        return result
    
    def _parse_from_filename_only(self, result: Dict[str, str]) -> Dict[str, str]:
        """Fallback parsing using only filename when path is unavailable or invalid"""
        
        name = result['parsed_name']
        
        # First try special complex filename patterns
        special_result = self._parse_special_complex_filename(name)
        if special_result['subject'] != 'unknown':
            # Merge special parsing results with existing result
            for key, value in special_result.items():
                if value != 'unknown' and value != 'æœªçŸ¥å‡ºç‰ˆç¤¾':
                    result[key] = value
            return result
        
        # Try to extract subject from filename
        subject = self._extract_subject_from_name(name)
        if subject:
            result['subject'] = subject
        
        # Try to extract grade from filename  
        grade = self._extract_grade_from_name(name)
        if grade:
            result['grade'] = grade
            
        # Try to extract semester from filename
        semester = self._extract_semester_from_name(name)
        if semester:
            result['semester'] = semester
        
        return result
    
    def _clean_publisher_name(self, publisher_raw: str) -> str:
        """Clean and normalize publisher names from path components"""
        if not publisher_raw:
            return 'æœªçŸ¥å‡ºç‰ˆç¤¾'
        
        # Handle combined publisher names like "åä¸­å¸ˆå¤§ç‰ˆ-åä¸­å¸ˆèŒƒå¤§å­¦å‡ºç‰ˆç¤¾"
        if '-' in publisher_raw:
            parts = publisher_raw.split('-')
            first_part = parts[0]
            
            # If first part ends with 'ç‰ˆ', use it (it's the short form)
            if first_part.endswith('ç‰ˆ'):
                return first_part
            else:
                # Otherwise use the second part and normalize it
                second_part = parts[-1]
                if second_part.endswith('å‡ºç‰ˆç¤¾'):
                    return second_part.replace('å‡ºç‰ˆç¤¾', 'ç‰ˆ')
                else:
                    return second_part + 'ç‰ˆ' if not second_part.endswith('ç‰ˆ') else second_part
        
        # Handle single publisher names
        publisher = publisher_raw
        
        # Normalize naming: prefer 'ç‰ˆ' ending over 'å‡ºç‰ˆç¤¾'
        if publisher.endswith('å‡ºç‰ˆç¤¾'):
            publisher = publisher.replace('å‡ºç‰ˆç¤¾', 'ç‰ˆ')
        elif not publisher.endswith('ç‰ˆ') and not publisher.endswith('ç¤¾'):
            publisher += 'ç‰ˆ'
        
        return publisher
    
    def _normalize_grade_format(self, grade: str) -> str:
        """Normalize grade format to use Chinese characters consistently"""
        # Convert Arabic numerals to Chinese characters for year grades
        arabic_to_chinese = {
            '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››', '5': 'äº”',
            '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹', '10': 'å'
        }
        
        # Handle patterns like "4å¹´çº§" -> "å››å¹´çº§"
        if re.match(r'\d+å¹´çº§', grade):
            number = re.search(r'(\d+)', grade).group(1)
            if number in arabic_to_chinese:
                return arabic_to_chinese[number] + 'å¹´çº§'
        
        # Handle patterns like "å¿…ä¿®1" -> keep as is (these are fine with numbers)
        # Handle patterns like "é€‰ä¿®2" -> keep as is
        
        return grade

    def _extract_grade_from_name(self, name: str) -> Optional[str]:
        """Extract grade information from filename with enhanced pattern recognition"""
        grade_patterns = [
            # Standard grade patterns
            r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]å¹´çº§)',
            r'(\d+å¹´çº§)',
            r'(å¿…ä¿®\d+)',
            r'(é€‰ä¿®\d+)',
            r'(å…¨ä¸€å†Œ)',
            
            # Enhanced patterns for special cases
            r'(ä½å¹´çº§)',      # For "å°å­¦ä½å¹´çº§" cases
            r'(é«˜å¹´çº§)',      # For "å°å­¦é«˜å¹´çº§" cases  
            r'(ä¸­å¹´çº§)',      # For "å°å­¦ä¸­å¹´çº§" cases
            r'(å¹¼å„¿å›­)',      # For kindergarten materials
            r'(å­¦å‰ç­)',      # For pre-school materials
            
            # High school specific patterns
            r'(é«˜ä¸€)',
            r'(é«˜äºŒ)', 
            r'(é«˜ä¸‰)',
            
            # Middle school specific patterns  
            r'(åˆä¸€)',
            r'(åˆäºŒ)',
            r'(åˆä¸‰)',
            r'(ä¸ƒå¹´çº§)',
            r'(å…«å¹´çº§)',
            r'(ä¹å¹´çº§)',
            
            # University course patterns
            r'(å¤§ä¸€)',
            r'(å¤§äºŒ)',
            r'(å¤§ä¸‰)',
            r'(å¤§å››)',
            
            # Special educational materials
            r'(ä¸­è€ƒ)',
            r'(é«˜è€ƒ)',
            r'(ç»ƒä¹ é¢˜)',
            r'(å¤ä¹ )',
            r'(é¢„ä¹ )',
            
            # Semester-based patterns
            r'(ä¸Šå­¦æœŸ)',
            r'(ä¸‹å­¦æœŸ)',
            r'(ç¬¬ä¸€å­¦æœŸ)',
            r'(ç¬¬äºŒå­¦æœŸ)',
        ]
        
        for pattern in grade_patterns:
            match = re.search(pattern, name)
            if match:
                grade = match.group(1)
                return self._normalize_grade_format(grade)
        return None
        
    def _extract_semester_from_name(self, name: str) -> Optional[str]:
        """Extract semester information from filename"""
        if 'ä¸Šå†Œ' in name:
            return 'first'
        elif 'ä¸‹å†Œ' in name:
            return 'second'
        elif 'å…¨ä¸€å†Œ' in name or 'å¿…ä¿®' in name or 'é€‰ä¿®' in name:
            return 'complete'
        return None
    
    def _extract_subject_from_name(self, name: str) -> Optional[str]:
        """Extract subject information from filename with enhanced pattern recognition"""
        
        # Special case patterns for complex titles
        special_patterns = [
            (r'ä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³å­¦ç”Ÿè¯»æœ¬', 'é“å¾·ä¸æ³•æ²»'),
            (r'æ€æƒ³å“å¾·', 'é“å¾·ä¸æ³•æ²»'),
            (r'å“å¾·ä¸ç”Ÿæ´»', 'é“å¾·ä¸æ³•æ²»'),
            (r'å“å¾·ä¸ç¤¾ä¼š', 'é“å¾·ä¸æ³•æ²»'),
            (r'æ€æƒ³æ”¿æ²»', 'æ”¿æ²»'),
            (r'ç”Ÿç‰©å­¦', 'ç”Ÿç‰©å­¦'),
            (r'ç”Ÿç‰©(?!å­¦)', 'ç”Ÿç‰©å­¦'),  # Match "ç”Ÿç‰©" but not "ç”Ÿç‰©å­¦"
        ]
        
        # Check special patterns first
        for pattern, subject in special_patterns:
            if re.search(pattern, name):
                return subject
        
        # Standard subject patterns
        subject_patterns = [
            r'(è¯­æ–‡)', r'(æ•°å­¦)', r'(è‹±è¯­)', r'(ç‰©ç†)', r'(åŒ–å­¦)', r'(ç”Ÿç‰©å­¦)', 
            r'(å†å²)', r'(åœ°ç†)', r'(æ”¿æ²»)', r'(é“å¾·ä¸æ³•æ²»)', r'(ç§‘å­¦)',
            r'(éŸ³ä¹)', r'(ç¾æœ¯)', r'(ä½“è‚²ä¸å¥åº·)', r'(ä¿¡æ¯æŠ€æœ¯)',
            r'(é«˜ç­‰æ•°å­¦)', r'(çº¿æ€§ä»£æ•°)', r'(æ¦‚ç‡è®º)', r'(å¤§å­¦ç‰©ç†)', r'(å¤§å­¦è‹±è¯­)',
            r'(è®¡ç®—æœº)', r'(æ€æƒ³å“å¾·)', r'(ç¤¾ä¼š)', r'(è‡ªç„¶)', 
            r'(ç»¼åˆå®è·µ)', r'(é€šç”¨æŠ€æœ¯)', r'(ä¿„è¯­)', r'(æ—¥è¯­)',
            r'(äººæ–‡åœ°ç†)', r'(å¿ƒç†å¥åº·)', r'(åŠ³åŠ¨æŠ€æœ¯)', r'(ä¹¦æ³•)',
            r'(ä¼ ç»Ÿæ–‡åŒ–)', r'(å›½å­¦)', r'(ç»å…¸è¯µè¯»)'
        ]
        
        for pattern in subject_patterns:
            match = re.search(pattern, name)
            if match:
                subject = match.group(1)
                return subject
        return None
    
    def _parse_special_complex_filename(self, name: str, path: str = None) -> Dict[str, str]:
        """Handle special complex filenames that don't fit standard patterns"""
        result = {
            'subject': 'unknown',
            'grade': 'unknown', 
            'semester': 'unknown',
            'level': 'unknown',
            'publisher': 'æœªçŸ¥å‡ºç‰ˆç¤¾'
        }
        
        # Special handling for "ä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³å­¦ç”Ÿè¯»æœ¬" series
        if 'ä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³å­¦ç”Ÿè¯»æœ¬' in name:
            result['subject'] = 'é“å¾·ä¸æ³•æ²»'
            result['publisher'] = 'äººæ°‘å‡ºç‰ˆç¤¾'  # This is typically published by People's Publishing House
            
            # Extract grade information
            if 'å°å­¦ä½å¹´çº§' in name:
                result['grade'] = 'ä½å¹´çº§'
                result['level'] = 'å°å­¦'
            elif 'å°å­¦é«˜å¹´çº§' in name:
                result['grade'] = 'é«˜å¹´çº§'
                result['level'] = 'å°å­¦'
            elif 'å°å­¦ä¸­å¹´çº§' in name:
                result['grade'] = 'ä¸­å¹´çº§'
                result['level'] = 'å°å­¦'
            elif 'åˆä¸­' in name:
                result['grade'] = 'å…¨å†Œ'
                result['level'] = 'åˆä¸­'
            elif 'é«˜ä¸­' in name:
                result['grade'] = 'å…¨å†Œ'
                result['level'] = 'é«˜ä¸­'
            elif 'å°å­¦' in name:
                result['grade'] = 'å…¨å†Œ'
                result['level'] = 'å°å­¦'
            
            # Try to extract level from path if not found in filename
            if result['level'] == 'unknown' and path:
                path_parts = path.split('/')
                if len(path_parts) > 0:
                    first_part = path_parts[0]
                    if first_part in ['å°å­¦', 'åˆä¸­', 'é«˜ä¸­', 'å¤§å­¦']:
                        result['level'] = first_part
                        if result['grade'] == 'unknown':
                            result['grade'] = 'å…¨å†Œ'
                
                result['semester'] = 'complete'
                return result
        
        # Handle other special cases here as needed
        # For example, traditional culture materials, special education books, etc.
        
        return result
    
    def process_tree_data(self, tree_data: List[Dict]) -> List[Dict]:
        """Process tree data to extract PDF file information with configuration filtering and URL testing"""
        textbooks = []
        pdf_count = 0
        ignored_count = 0
        total_files = len(tree_data)
        
        print(f"ğŸ” Processing {total_files} repository tree entries to find PDF files...")
        
        # First pass: collect all PDF files and create basic entries
        url_test_data = []  # For batch URL testing
        
        for item in tree_data:
            path = item.get('path', '')
            item_type = item.get('type', '')
            file_size = item.get('size', 0)
            
            # Only process PDF files (blobs in git terminology)
            # Include regular PDFs and split files (.pdf.1, .pdf.2, etc.)
            is_pdf = (path.endswith('.pdf') or path.endswith('.PDF'))
            is_split_pdf = bool(re.search(r'\.pdf\.\d+$', path, re.IGNORECASE))
            
            if item_type == 'blob' and (is_pdf or is_split_pdf):
                pdf_count += 1
                
                # Check if this path should be ignored based on configuration
                level_key = self.get_level_key_from_path(path)
                if level_key and self.should_ignore_path(path, level_key):
                    ignored_count += 1
                    continue
                
                # Extract filename from path
                pdf_filename = os.path.basename(path)
                
                # Parse file information - function handles both regular and math practice materials
                parsed = self._parse_textbook_metadata(pdf_filename, path)
                
                # Create GitHub raw URL
                github_url = f"{self.raw_base_url}/{urllib.parse.quote(path)}"
                
                textbook_entry = {
                    'level': parsed['level'],
                    'subject': parsed['subject'] or 'unknown',
                    'grade': parsed['grade'] or 'unknown',
                    'semester': parsed['semester'] or 'unknown',
                    'publisher': parsed['publisher'],
                    'title': parsed['parsed_name'],
                    'file_path': path,
                    'file_name': pdf_filename,
                    'download_url': github_url,  # Keep for backward compatibility
                    'international_url': github_url,  # International users always use GitHub direct
                    'china_url': github_url,  # Will be updated after URL testing
                    'is_split': parsed['is_split'],
                    'part_number': parsed['part_number'],
                    'file_size': file_size
                }
                
                textbooks.append(textbook_entry)
                
                # Collect data for URL testing
                if self.test_urls:
                    url_test_data.append((github_url, path))
        
        print(f"âœ… Found and processed {pdf_count} PDF files out of {total_files} total repository entries")
        if ignored_count > 0:
            print(f"ğŸš« Ignored {ignored_count} files based on path patterns")
        print(f"ğŸ“Š Generated {len(textbooks)} textbook entries (each split part shown separately)")
        
        # Second pass: test URLs and update china_url field
        if self.test_urls and url_test_data and self.url_tester:
            print(f"\nğŸ”— Starting URL testing for {len(url_test_data)} PDF files...")
            url_results = self.url_tester.test_multiple_urls(url_test_data)
            
            # Update textbook entries with tested URLs
            for textbook in textbooks:
                github_url = textbook['download_url']
                if github_url in url_results:
                    result_data = url_results[github_url]
                    textbook['china_url'] = result_data['urls']['china_url']
                    # Add testing metadata for debugging
                    textbook['jsdelivr_works'] = result_data['jsdelivr_works']
                    textbook['test_status_code'] = result_data['status_code']
            
            print(f"âœ… URL testing completed and china_url fields updated")
        
        return textbooks

def generate_javascript_file(textbook_data: List[Dict], output_file: str = 'textbook-data.js'):
    """Generate JavaScript file with comprehensive textbook data including location-aware URLs"""
    
    # Generate statistics
    stats = {
        'total_entries': len(textbook_data),
        'main_files': len([t for t in textbook_data if not t['is_split']]),
        'split_files': len([t for t in textbook_data if t['is_split']]),
        'levels': len(set(t['level'] for t in textbook_data)),
        'subjects': len(set(t['subject'] for t in textbook_data)),
        'publishers': len(set(t['publisher'] for t in textbook_data))
    }
    
    # Add URL testing statistics if available
    tested_entries = [t for t in textbook_data if 'jsdelivr_works' in t]
    if tested_entries:
        jsdelivr_working = len([t for t in tested_entries if t.get('jsdelivr_works', False)])
        jsdelivr_failed = len(tested_entries) - jsdelivr_working
        stats.update({
            'url_tested': len(tested_entries),
            'jsdelivr_working': jsdelivr_working,
            'jsdelivr_failed': jsdelivr_failed,
            'jsdelivr_success_rate': round((jsdelivr_working / len(tested_entries) * 100), 1) if tested_entries else 0
        })
    
    js_content = f"""// Dynamic Chinese Textbook Data with Location-Aware URLs
// Auto-generated from GitHub repository: https://github.com/TapXWorld/ChinaTextbook
// Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
// Statistics: {json.dumps(stats, ensure_ascii=False)}
//
// URL Structure:
// - international_url: GitHub direct (for international users)
// - china_url: jsDelivr CDN or ghfast.top fallback (for China users)
// - download_url: GitHub direct (backward compatibility)

const TEXTBOOK_DATA = {json.dumps(textbook_data, ensure_ascii=False, indent=2)};

// Create TextbookData class for easy data access
class TextbookData {{
    constructor(data) {{
        this.data = data;
    }}
    
    getLevels() {{
        return [...new Set(this.data.map(book => book.level))];
    }}
    
    getSubjects(level) {{
        return [...new Set(this.data.filter(book => book.level === level).map(book => book.subject))];
    }}
    
    getGrades(level, subject) {{
        return [...new Set(this.data.filter(book => book.level === level && book.subject === subject).map(book => book.grade))];
    }}
    
    filter(criteria) {{
        return this.data.filter(book => {{
            return Object.keys(criteria).every(key => book[key] === criteria[key]);
        }});
    }}
    
    getStats() {{
        return {json.dumps(stats, ensure_ascii=False)};
    }}
}}

// Create global instance
window.textbookData = new TextbookData(TEXTBOOK_DATA);
"""

    # Write to file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(js_content)
        
    return output_file, stats

def main():
    print("ğŸš€ Dynamic Chinese Textbook Data Generator")
    print("ğŸ“¡ Specifically designed for TapXWorld/ChinaTextbook repository")
    print("ğŸ“š Using GitHub Trees API for efficient data retrieval...")
    
    # Initialize scanner - hardcoded for TapXWorld/ChinaTextbook repository
    # You can optionally provide a GitHub token for higher rate limits:
    # scanner = GitHubTextbookScanner(token="your_github_token_here")
    scanner = GitHubTextbookScanner()
    
    try:
        # Get repository tree structure
        tree_data = scanner.get_repository_tree()
        
        if not tree_data:
            print("âŒ Failed to fetch repository tree. Exiting.")
            return
        
        # Process tree data to extract PDF information
        raw_textbooks = scanner.process_tree_data(tree_data)
        
        # Filter out unknown entries for cleaner data
        print(f"ğŸ§¹ Filtering out entries with unknown level or subject...")
        unknown_before = len([book for book in raw_textbooks if book['level'] == 'unknown' or book['subject'] == 'unknown'])
        
        filtered_data = [
            book for book in raw_textbooks 
            if book['level'] != 'unknown' and book['subject'] != 'unknown'
        ]
        
        print(f"ğŸ“Š Filtered out {unknown_before} entries with unknown data")
        print(f"âœ… Final processed textbook entries: {len(filtered_data)}")
        
        # Generate JavaScript file
        print("ğŸ“ Generating JavaScript data file...")
        output_file, stats = generate_javascript_file(filtered_data)
        
        print(f"ğŸ‰ Successfully generated {output_file}")
        print(f"ğŸ“Š Statistics:")
        for key, value in stats.items():
            if key.startswith('jsdelivr_'):
                print(f"   ğŸ”— {key}: {value}")
            else:
                print(f"   ğŸ“š {key}: {value}")
        
        # Show URL testing summary if available
        if 'url_tested' in stats:
            print(f"\nğŸ”— URL Testing Summary:")
            print(f"   âœ… jsDelivr working: {stats['jsdelivr_working']}")
            print(f"   âŒ jsDelivr failed: {stats['jsdelivr_failed']}")
            print(f"   ğŸ“ˆ Success rate: {stats['jsdelivr_success_rate']}%")
            print(f"   ğŸ’¡ China users will get jsDelivr for working URLs, ghfast.top fallback for failed ones")
            print(f"   ğŸŒ International users always get GitHub direct URLs")
        
        # Show sample data
        print(f"\nğŸ” Sample entries:")
        for i, entry in enumerate(filtered_data[:5]):
            split_info = f" (part {entry['part_number']})" if entry.get('is_split') else ""
            jsdelivr_status = ""
            if 'jsdelivr_works' in entry:
                jsdelivr_status = " âœ…" if entry['jsdelivr_works'] else " âŒ"
            print(f"   {i+1}. {entry['level']}/{entry['subject']}/{entry['grade']} - {entry['publisher']}{split_info}{jsdelivr_status}")
        
        print(f"\nğŸ’¡ The generated file can be used directly in your web application!")
        print(f"   Include it with: <script src='{output_file}'></script>")
        print(f"   Access URLs: entry.international_url (international) or entry.china_url (China)")
        
        # Show unique levels, subjects, and publishers found
        levels = sorted(set(book['level'] for book in filtered_data if book['level'] != 'unknown'))
        subjects = sorted(set(book['subject'] for book in filtered_data if book['subject'] != 'unknown'))
        publishers = sorted(set(book['publisher'] for book in filtered_data if book['publisher'] != 'æœªçŸ¥å‡ºç‰ˆç¤¾'))
        
        print(f"\nğŸ“‹ Found data structure:")
        print(f"   Levels: {levels}")
        print(f"   Subjects: {subjects[:10]}{'...' if len(subjects) > 10 else ''}")
        print(f"   Publishers: {publishers[:10]}{'...' if len(publishers) > 10 else ''}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Process interrupted by user")
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main() 